{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421e5259",
   "metadata": {},
   "source": [
    "# Semantic Scholar Data Import Notebook\n",
    "\n",
    "This notebook demonstrates how to import and work with data from the Semantic Scholar Academic Graph API. Semantic Scholar provides comprehensive academic paper data with advanced features like influence metrics, citation contexts, and AI-powered paper recommendations.\n",
    "\n",
    "## Semantic Scholar API Overview\n",
    "- **Base URL**: https://api.semanticscholar.org/graph/v1/\n",
    "- **Authentication**: API key required for higher rate limits\n",
    "- **Rate Limiting**: 100 requests/5 minutes (free), 1000 requests/5 minutes (with API key)\n",
    "- **Coverage**: 200M+ papers across all fields of science\n",
    "\n",
    "## What we'll cover:\n",
    "1. API setup and authentication\n",
    "2. Fetching papers with detailed metadata\n",
    "3. Author information and collaboration networks\n",
    "4. CitedBy analysis and contexts\n",
    "5. Recommendations and related papers\n",
    "6. Advanced search and filtering\n",
    "7. Data processing and export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb577c2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66898d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for API interaction and data handling\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Any, Union\n",
    "from urllib.parse import urljoin, urlencode, quote\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data visualization and analysis\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    print(\"Visualization libraries loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"Visualization libraries not available - install matplotlib, seaborn, numpy for plots\")\n",
    "\n",
    "# For text processing\n",
    "try:\n",
    "    import re\n",
    "    from collections import Counter, defaultdict\n",
    "    print(\"Text processing libraries loaded\")\n",
    "except ImportError:\n",
    "    print(\"Some text processing libraries not available\")\n",
    "\n",
    "print(\"All core libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa320a7",
   "metadata": {},
   "source": [
    "## 2. API Configuration and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Scholar API Configuration\n",
    "class SemanticScholarConfig:\n",
    "    \"\"\"Configuration class for Semantic Scholar API\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/v1\"\n",
    "    \n",
    "    # Get your API key from: https://www.semanticscholar.org/product/api#api-key\n",
    "    # Without API key: 100 requests per 5 minutes\n",
    "    # With API key: 1000 requests per 5 minutes\n",
    "    API_KEY = None  # Set your API key here or use environment variable\n",
    "    \n",
    "    # Request headers\n",
    "    @property\n",
    "    def headers(self):\n",
    "        headers = {\n",
    "            \"User-Agent\": \"KnowledgeFabric/1.0 (mailto:your-email@domain.com)\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        if self.API_KEY:\n",
    "            headers[\"x-pipelines-key\"] = self.API_KEY\n",
    "        return headers\n",
    "    \n",
    "    # Rate limiting based on API key availability\n",
    "    @property\n",
    "    def request_delay(self):\n",
    "        if self.API_KEY:\n",
    "            return 0.31  # ~3 requests per second (1000/5min = 3.33/sec)\n",
    "        else:\n",
    "            return 3.1   # ~0.33 requests per second (100/5min = 0.33/sec)\n",
    "    \n",
    "    # Common field sets for different queries\n",
    "    PAPER_FIELDS = [\n",
    "        \"paperId\", \"externalIds\", \"title\", \"abstract\", \"venue\", \"year\", \n",
    "        \"referenceCount\", \"citationCount\", \"influentialCitationCount\",\n",
    "        \"isOpenAccess\", \"openAccessPdf\", \"fieldsOfStudy\", \"s2FieldsOfStudy\",\n",
    "        \"authors\", \"citations\", \"references\", \"embedding\", \"tldr\"\n",
    "    ]\n",
    "    \n",
    "    AUTHOR_FIELDS = [\n",
    "        \"authorId\", \"externalIds\", \"name\", \"aliases\", \"affiliations\",\n",
    "        \"homepage\", \"paperCount\", \"citationCount\", \"hIndex\", \"papers\"\n",
    "    ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = SemanticScholarConfig()\n",
    "\n",
    "# Check if API key is set (you can set it here or via environment variable)\n",
    "import os\n",
    "if not config.API_KEY:\n",
    "    config.API_KEY = os.getenv('SEMANTIC_SCHOLAR_API_KEY')\n",
    "\n",
    "print(f\"Semantic Scholar API Base URL: {config.BASE_URL}\")\n",
    "print(f\"API Key configured: {'Yes' if config.API_KEY else 'No (using free tier)'}\")\n",
    "print(f\"Request delay: {config.request_delay:.2f} seconds\")\n",
    "print(f\"Rate limit: {'1000 req/5min' if config.API_KEY else '100 req/5min'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a86256",
   "metadata": {},
   "source": [
    "## 3. Core API Request Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c25755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_s2_request(endpoint: str, params: Dict = None, delay: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Make a request to the Semantic Scholar API with error handling and rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        endpoint: API endpoint (e.g., 'paper/search', 'author/search')\n",
    "        params: Query parameters\n",
    "        delay: Whether to add delay for rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        JSON response as dictionary\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Build URL\n",
    "    url = urljoin(config.BASE_URL, endpoint)\n",
    "    \n",
    "    try:\n",
    "        # Rate limiting\n",
    "        if delay:\n",
    "            time.sleep(config.request_delay)\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(url, headers=config.headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request to {url}: {e}\")\n",
    "        if hasattr(e.response, 'status_code'):\n",
    "            print(f\"Status code: {e.response.status_code}\")\n",
    "            if e.response.status_code == 429:\n",
    "                print(\"Rate limit exceeded. Consider adding an API key or increasing delays.\")\n",
    "        return None\n",
    "\n",
    "def test_s2_connection():\n",
    "    \"\"\"Test the Semantic Scholar API connection\"\"\"\n",
    "    print(\"Testing Semantic Scholar API connection...\")\n",
    "    \n",
    "    # Test with a simple paper search\n",
    "    response = make_s2_request(\"paper/search\", {\n",
    "        \"query\": \"attention is all you need\",\n",
    "        \"limit\": 1,\n",
    "        \"fields\": \"title,year,authors,citationCount\"\n",
    "    })\n",
    "    \n",
    "    if response and \"data\" in response:\n",
    "        paper = response[\"data\"][0] if response[\"data\"] else None\n",
    "        if paper:\n",
    "            print(\"✅ API connection successful!\")\n",
    "            print(f\"Test paper: {paper.get('title', 'Unknown')}\")\n",
    "            print(f\"Citations: {paper.get('citationCount', 0):,}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ API connection failed - no data returned\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"❌ API connection failed!\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_s2_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802336b",
   "metadata": {},
   "source": [
    "## 4. Paper Search and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_papers(query: str, limit: int = 100, fields: List[str] = None, \n",
    "                  year_filter: str = None, venue_filter: str = None,\n",
    "                  field_of_study: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search for papers using Semantic Scholar API.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query string\n",
    "        limit: Maximum number of results\n",
    "        fields: List of fields to retrieve\n",
    "        year_filter: Year range filter (e.g., \"2020-2023\")\n",
    "        venue_filter: Venue name filter\n",
    "        field_of_study: Field of study filter\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with paper data\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\"paperId\", \"title\", \"abstract\", \"year\", \"authors\", \"venue\",\n",
    "                 \"citationCount\", \"influentialCitationCount\", \"isOpenAccess\", \n",
    "                 \"fieldsOfStudy\", \"tldr\"]\n",
    "    \n",
    "    papers_data = []\n",
    "    offset = 0\n",
    "    batch_size = min(100, limit)  # API max is 100 per request\n",
    "    \n",
    "    print(f\"Searching for papers: '{query}' (limit: {limit})\")\n",
    "    \n",
    "    while offset < limit:\n",
    "        current_batch = min(batch_size, limit - offset)\n",
    "        \n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": current_batch,\n",
    "            \"offset\": offset,\n",
    "            \"fields\": \",\".join(fields)\n",
    "        }\n",
    "        \n",
    "        # Add optional filters\n",
    "        if year_filter:\n",
    "            params[\"year\"] = year_filter\n",
    "        if venue_filter:\n",
    "            params[\"venue\"] = venue_filter\n",
    "        if field_of_study:\n",
    "            params[\"fieldsOfStudy\"] = field_of_study\n",
    "        \n",
    "        response = make_s2_request(\"paper/search\", params)\n",
    "        \n",
    "        if not response or \"data\" not in response:\n",
    "            print(f\"Failed to fetch papers at offset {offset}\")\n",
    "            break\n",
    "        \n",
    "        batch_papers = response[\"data\"]\n",
    "        if not batch_papers:\n",
    "            print(f\"No more papers found at offset {offset}\")\n",
    "            break\n",
    "        \n",
    "        # Process each paper\n",
    "        for paper in batch_papers:\n",
    "            paper_data = {\n",
    "                \"paper_id\": paper.get(\"paperId\"),\n",
    "                \"title\": paper.get(\"title\"),\n",
    "                \"abstract\": paper.get(\"abstract\"),\n",
    "                \"year\": paper.get(\"year\"),\n",
    "                \"venue\": paper.get(\"venue\"),\n",
    "                \"citation_count\": paper.get(\"citationCount\", 0),\n",
    "                \"influential_citation_count\": paper.get(\"influentialCitationCount\", 0),\n",
    "                \"is_open_access\": paper.get(\"isOpenAccess\", False),\n",
    "                \"fields_of_study\": paper.get(\"fieldsOfStudy\", []),\n",
    "                \"tldr\": paper.get(\"tldr\", {}).get(\"text\") if paper.get(\"tldr\") else None,\n",
    "                \"author_count\": len(paper.get(\"authors\", [])),\n",
    "                \"author_names\": [auth.get(\"name\") for auth in paper.get(\"authors\", []) if auth.get(\"name\")],\n",
    "                \"first_author\": paper.get(\"authors\", [{}])[0].get(\"name\") if paper.get(\"authors\") else None\n",
    "            }\n",
    "            papers_data.append(paper_data)\n",
    "        \n",
    "        offset += current_batch\n",
    "        print(f\"Fetched {len(papers_data)} papers so far...\")\n",
    "        \n",
    "        # Check if we've reached the total available\n",
    "        total = response.get(\"total\", 0)\n",
    "        if total > 0 and len(papers_data) >= total:\n",
    "            print(f\"Reached total available papers: {total}\")\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(papers_data)\n",
    "    print(f\"Search completed. Retrieved {len(df)} papers.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_paper_details(paper_id: str, include_citations: bool = False, \n",
    "                     include_references: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific paper.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: Semantic Scholar paper ID or DOI\n",
    "        include_citations: Whether to include citation data\n",
    "        include_references: Whether to include reference data\n",
    "    \n",
    "    Returns:\n",
    "        Detailed paper information\n",
    "    \"\"\"\n",
    "    fields = [\"paperId\", \"externalIds\", \"title\", \"abstract\", \"venue\", \"year\",\n",
    "             \"referenceCount\", \"citationCount\", \"influentialCitationCount\",\n",
    "             \"isOpenAccess\", \"openAccessPdf\", \"fieldsOfStudy\", \"s2FieldsOfStudy\",\n",
    "             \"authors\", \"embedding\", \"tldr\"]\n",
    "    \n",
    "    if include_citations:\n",
    "        fields.append(\"citations\")\n",
    "    if include_references:\n",
    "        fields.append(\"references\")\n",
    "    \n",
    "    params = {\"fields\": \",\".join(fields)}\n",
    "    \n",
    "    response = make_s2_request(f\"paper/{paper_id}\", params)\n",
    "    return response\n",
    "\n",
    "# Example: Search for recent transformer papers\n",
    "print(\"Example 1: Searching for transformer papers...\")\n",
    "transformer_papers = search_papers(\n",
    "    query=\"transformer attention mechanism\",\n",
    "    limit=50,\n",
    "    year_filter=\"2020-2024\",\n",
    "    field_of_study=\"Computer Science\"\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(transformer_papers)} transformer papers:\")\n",
    "print(transformer_papers[[\"title\", \"year\", \"citation_count\", \"venue\", \"is_open_access\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cef450",
   "metadata": {},
   "source": [
    "## 5. Author Information and Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f325b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_authors(query: str, limit: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Search for authors using Semantic Scholar API.\n",
    "    \n",
    "    Args:\n",
    "        query: Author name or affiliation search query\n",
    "        limit: Maximum number of results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with author data\n",
    "    \"\"\"\n",
    "    authors_data = []\n",
    "    offset = 0\n",
    "    batch_size = min(100, limit)\n",
    "    \n",
    "    print(f\"Searching for authors: '{query}' (limit: {limit})\")\n",
    "    \n",
    "    while offset < limit:\n",
    "        current_batch = min(batch_size, limit - offset)\n",
    "        \n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"limit\": current_batch,\n",
    "            \"offset\": offset,\n",
    "            \"fields\": \",\".join(config.AUTHOR_FIELDS)\n",
    "        }\n",
    "        \n",
    "        response = make_s2_request(\"author/search\", params)\n",
    "        \n",
    "        if not response or \"data\" not in response:\n",
    "            print(f\"Failed to fetch authors at offset {offset}\")\n",
    "            break\n",
    "        \n",
    "        batch_authors = response[\"data\"]\n",
    "        if not batch_authors:\n",
    "            break\n",
    "        \n",
    "        # Process each author\n",
    "        for author in batch_authors:\n",
    "            # Get current affiliation\n",
    "            affiliations = author.get(\"affiliations\", [])\n",
    "            current_affiliation = affiliations[0] if affiliations else None\n",
    "            \n",
    "            author_data = {\n",
    "                \"author_id\": author.get(\"authorId\"),\n",
    "                \"name\": author.get(\"name\"),\n",
    "                \"aliases\": author.get(\"aliases\", []),\n",
    "                \"paper_count\": author.get(\"paperCount\", 0),\n",
    "                \"citation_count\": author.get(\"citationCount\", 0),\n",
    "                \"h_index\": author.get(\"hIndex\", 0),\n",
    "                \"homepage\": author.get(\"homepage\"),\n",
    "                \"current_affiliation\": current_affiliation,\n",
    "                \"external_ids\": author.get(\"externalIds\", {}),\n",
    "                \"orcid\": author.get(\"externalIds\", {}).get(\"ORCID\"),\n",
    "                \"google_scholar\": author.get(\"externalIds\", {}).get(\"GoogleScholar\")\n",
    "            }\n",
    "            authors_data.append(author_data)\n",
    "        \n",
    "        offset += current_batch\n",
    "        print(f\"Fetched {len(authors_data)} authors so far...\")\n",
    "    \n",
    "    df = pd.DataFrame(authors_data)\n",
    "    print(f\"Author search completed. Retrieved {len(df)} authors.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_author_details(author_id: str, include_papers: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Get detailed information for a specific author.\n",
    "    \n",
    "    Args:\n",
    "        author_id: Semantic Scholar author ID\n",
    "        include_papers: Whether to include author's papers\n",
    "    \n",
    "    Returns:\n",
    "        Detailed author information\n",
    "    \"\"\"\n",
    "    fields = config.AUTHOR_FIELDS.copy()\n",
    "    if not include_papers and \"papers\" in fields:\n",
    "        fields.remove(\"papers\")\n",
    "    \n",
    "    params = {\"fields\": \",\".join(fields)}\n",
    "    \n",
    "    response = make_s2_request(f\"author/{author_id}\", params)\n",
    "    return response\n",
    "\n",
    "def analyze_collaboration_network(papers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze collaboration networks from papers DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame containing papers with author information\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with collaboration statistics\n",
    "    \"\"\"\n",
    "    collaborations = []\n",
    "    \n",
    "    for _, paper in papers_df.iterrows():\n",
    "        authors = paper.get(\"author_names\", [])\n",
    "        if len(authors) > 1:\n",
    "            # Count collaborations between all pairs of authors\n",
    "            for i, author1 in enumerate(authors):\n",
    "                for author2 in authors[i+1:]:\n",
    "                    collaborations.append({\n",
    "                        \"author1\": author1,\n",
    "                        \"author2\": author2,\n",
    "                        \"paper_title\": paper.get(\"title\"),\n",
    "                        \"year\": paper.get(\"year\"),\n",
    "                        \"citation_count\": paper.get(\"citation_count\", 0)\n",
    "                    })\n",
    "    \n",
    "    if not collaborations:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    collab_df = pd.DataFrame(collaborations)\n",
    "    \n",
    "    # Aggregate collaboration statistics\n",
    "    collab_stats = collab_df.groupby([\"author1\", \"author2\"]).agg({\n",
    "        \"paper_title\": \"count\",\n",
    "        \"citation_count\": [\"sum\", \"mean\"]\n",
    "    }).round(2)\n",
    "    \n",
    "    collab_stats.columns = [\"collaboration_count\", \"total_citations\", \"avg_citations\"]\n",
    "    collab_stats = collab_stats.reset_index()\n",
    "    \n",
    "    return collab_stats.sort_values(\"collaboration_count\", ascending=False)\n",
    "\n",
    "# Example: Search for AI researchers\n",
    "print(\"Example 2: Searching for AI researchers...\")\n",
    "ai_researchers = search_authors(\"artificial intelligence\", limit=25)\n",
    "\n",
    "print(f\"\\nFound {len(ai_researchers)} AI researchers:\")\n",
    "print(ai_researchers[[\"name\", \"paper_count\", \"citation_count\", \"h_index\", \"current_affiliation\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd1177",
   "metadata": {},
   "source": "## 6. CitedBy Analysis and Contexts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539da641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paper_citations(paper_id: str, limit: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get citations for a specific paper with citation contexts.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: Semantic Scholar paper ID\n",
    "        limit: Maximum number of citations to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with citation information\n",
    "    \"\"\"\n",
    "    citations_data = []\n",
    "    offset = 0\n",
    "    batch_size = min(1000, limit)  # API allows large batches for citations\n",
    "    \n",
    "    print(f\"Fetching citations for paper {paper_id}...\")\n",
    "    \n",
    "    while offset < limit:\n",
    "        params = {\n",
    "            \"offset\": offset,\n",
    "            \"limit\": min(batch_size, limit - offset),\n",
    "            \"fields\": \"paperId,title,year,authors,citationCount,contexts\"\n",
    "        }\n",
    "        \n",
    "        response = make_s2_request(f\"paper/{paper_id}/citations\", params)\n",
    "        \n",
    "        if not response or \"data\" not in response:\n",
    "            print(f\"Failed to fetch citations at offset {offset}\")\n",
    "            break\n",
    "        \n",
    "        batch_citations = response[\"data\"]\n",
    "        if not batch_citations:\n",
    "            break\n",
    "        \n",
    "        # Process each citation\n",
    "        for citation in batch_citations:\n",
    "            citing_paper = citation.get(\"citingPaper\", {})\n",
    "            contexts = citation.get(\"contexts\", [])\n",
    "            \n",
    "            citation_data = {\n",
    "                \"citing_paper_id\": citing_paper.get(\"paperId\"),\n",
    "                \"citing_title\": citing_paper.get(\"title\"),\n",
    "                \"citing_year\": citing_paper.get(\"year\"),\n",
    "                \"citing_citation_count\": citing_paper.get(\"citationCount\", 0),\n",
    "                \"citing_authors\": [auth.get(\"name\") for auth in citing_paper.get(\"authors\", [])],\n",
    "                \"citation_contexts\": [ctx for ctx in contexts if ctx],\n",
    "                \"context_count\": len([ctx for ctx in contexts if ctx])\n",
    "            }\n",
    "            citations_data.append(citation_data)\n",
    "        \n",
    "        offset += len(batch_citations)\n",
    "        print(f\"Fetched {len(citations_data)} citations so far...\")\n",
    "        \n",
    "        # Check if we've reached the end\n",
    "        if len(batch_citations) < batch_size:\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(citations_data)\n",
    "    print(f\"CitedBy fetch completed. Retrieved {len(df)} citations.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_citation_contexts(citations_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze citation contexts to understand how papers are being cited.\n",
    "    \n",
    "    Args:\n",
    "        citations_df: DataFrame with citation context data\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with citation context analysis\n",
    "    \"\"\"\n",
    "    if citations_df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_citations = len(citations_df)\n",
    "    citations_with_context = citations_df[citations_df[\"context_count\"] > 0]\n",
    "    context_rate = len(citations_with_context) / total_citations if total_citations > 0 else 0\n",
    "    \n",
    "    # Temporal analysis\n",
    "    yearly_citations = citations_df.groupby(\"citing_year\").size().to_dict()\n",
    "    \n",
    "    # Context analysis\n",
    "    all_contexts = []\n",
    "    for contexts in citations_df[\"citation_contexts\"]:\n",
    "        all_contexts.extend(contexts)\n",
    "    \n",
    "    # Simple keyword analysis from contexts\n",
    "    context_keywords = Counter()\n",
    "    for context in all_contexts:\n",
    "        if context and len(context) > 10:  # Filter very short contexts\n",
    "            # Simple keyword extraction (you could use more sophisticated NLP)\n",
    "            words = re.findall(r'\\b[a-zA-Z]{4,}\\b', context.lower())\n",
    "            context_keywords.update(words)\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_citations\": total_citations,\n",
    "        \"citations_with_context\": len(citations_with_context),\n",
    "        \"context_coverage_rate\": context_rate,\n",
    "        \"yearly_distribution\": yearly_citations,\n",
    "        \"avg_contexts_per_citation\": citations_df[\"context_count\"].mean(),\n",
    "        \"top_context_keywords\": dict(context_keywords.most_common(10)),\n",
    "        \"citation_years_range\": (citations_df[\"citing_year\"].min(), citations_df[\"citing_year\"].max()) if total_citations > 0 else None\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def get_paper_references(paper_id: str, limit: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get references for a specific paper.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: Semantic Scholar paper ID\n",
    "        limit: Maximum number of references to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with reference information\n",
    "    \"\"\"\n",
    "    references_data = []\n",
    "    offset = 0\n",
    "    batch_size = min(1000, limit)\n",
    "    \n",
    "    print(f\"Fetching references for paper {paper_id}...\")\n",
    "    \n",
    "    while offset < limit:\n",
    "        params = {\n",
    "            \"offset\": offset,\n",
    "            \"limit\": min(batch_size, limit - offset),\n",
    "            \"fields\": \"paperId,title,year,authors,citationCount,venue\"\n",
    "        }\n",
    "        \n",
    "        response = make_s2_request(f\"paper/{paper_id}/references\", params)\n",
    "        \n",
    "        if not response or \"data\" not in response:\n",
    "            break\n",
    "        \n",
    "        batch_references = response[\"data\"]\n",
    "        if not batch_references:\n",
    "            break\n",
    "        \n",
    "        # Process each reference\n",
    "        for reference in batch_references:\n",
    "            cited_paper = reference.get(\"citedPaper\", {})\n",
    "            \n",
    "            ref_data = {\n",
    "                \"referenced_paper_id\": cited_paper.get(\"paperId\"),\n",
    "                \"referenced_title\": cited_paper.get(\"title\"),\n",
    "                \"referenced_year\": cited_paper.get(\"year\"),\n",
    "                \"referenced_citation_count\": cited_paper.get(\"citationCount\", 0),\n",
    "                \"referenced_venue\": cited_paper.get(\"venue\"),\n",
    "                \"referenced_authors\": [auth.get(\"name\") for auth in cited_paper.get(\"authors\", [])]\n",
    "            }\n",
    "            references_data.append(ref_data)\n",
    "        \n",
    "        offset += len(batch_references)\n",
    "        \n",
    "        if len(batch_references) < batch_size:\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame(references_data)\n",
    "    print(f\"Reference fetch completed. Retrieved {len(df)} references.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example: Analyze citations for a highly cited paper\n",
    "if 'transformer_papers' in locals() and len(transformer_papers) > 0:\n",
    "    # Get the most cited paper from our search\n",
    "    most_cited = transformer_papers.loc[transformer_papers[\"citation_count\"].idxmax()]\n",
    "    print(f\"\\nExample 3: Analyzing citations for '{most_cited['title']}'\")\n",
    "    print(f\"Paper has {most_cited['citation_count']:,} citations\")\n",
    "    \n",
    "    # Get some citations (limit to 50 for demo)\n",
    "    citations = get_paper_citations(most_cited[\"paper_id\"], limit=50)\n",
    "    \n",
    "    if not citations.empty:\n",
    "        print(f\"\\nSample of {len(citations)} citations:\")\n",
    "        print(citations[[\"citing_title\", \"citing_year\", \"citing_citation_count\", \"context_count\"]].head())\n",
    "        \n",
    "        # Analyze citation contexts\n",
    "        context_analysis = analyze_citation_contexts(citations)\n",
    "        print(f\"\\nCitedBy Context Analysis:\")\n",
    "        print(f\"- Citations with context: {context_analysis.get('citations_with_context', 0)}\")\n",
    "        print(f\"- Context coverage: {context_analysis.get('context_coverage_rate', 0):.2%}\")\n",
    "        print(f\"- Avg contexts per citation: {context_analysis.get('avg_contexts_per_citation', 0):.1f}\")\n",
    "else:\n",
    "    print(\"No transformer papers available for citation analysis. Run the paper search cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ab2ba",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Related Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3743273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(paper_id: str, limit: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get paper recommendations based on a seed paper.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: Semantic Scholar paper ID to base recommendations on\n",
    "        limit: Maximum number of recommendations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with recommended papers\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"fields\": \"paperId,title,year,authors,abstract,citationCount,influentialCitationCount,venue\",\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    \n",
    "    response = make_s2_request(f\"recommendations/v1/papers/forpaper/{paper_id}\", params)\n",
    "    \n",
    "    if not response or \"recommendedPapers\" not in response:\n",
    "        print(\"Failed to fetch recommendations\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    recommendations_data = []\n",
    "    \n",
    "    for paper in response[\"recommendedPapers\"]:\n",
    "        rec_data = {\n",
    "            \"paper_id\": paper.get(\"paperId\"),\n",
    "            \"title\": paper.get(\"title\"),\n",
    "            \"year\": paper.get(\"year\"),\n",
    "            \"venue\": paper.get(\"venue\"),\n",
    "            \"citation_count\": paper.get(\"citationCount\", 0),\n",
    "            \"influential_citation_count\": paper.get(\"influentialCitationCount\", 0),\n",
    "            \"author_count\": len(paper.get(\"authors\", [])),\n",
    "            \"authors\": [auth.get(\"name\") for auth in paper.get(\"authors\", [])],\n",
    "            \"has_abstract\": bool(paper.get(\"abstract\"))\n",
    "        }\n",
    "        recommendations_data.append(rec_data)\n",
    "    \n",
    "    df = pd.DataFrame(recommendations_data)\n",
    "    print(f\"Retrieved {len(df)} recommendations\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def batch_search_by_ids(paper_ids: List[str], fields: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve multiple papers by their IDs in batch.\n",
    "    \n",
    "    Args:\n",
    "        paper_ids: List of paper IDs\n",
    "        fields: Fields to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with paper information\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        fields = [\"paperId\", \"title\", \"year\", \"authors\", \"citationCount\", \"venue\"]\n",
    "    \n",
    "    papers_data = []\n",
    "    \n",
    "    # Process in batches (API supports multiple IDs in one request)\n",
    "    batch_size = 500  # Semantic Scholar allows large batches\n",
    "    \n",
    "    for i in range(0, len(paper_ids), batch_size):\n",
    "        batch_ids = paper_ids[i:i+batch_size]\n",
    "        \n",
    "        # Join IDs with commas\n",
    "        ids_param = \",\".join(batch_ids)\n",
    "        \n",
    "        params = {\"fields\": \",\".join(fields)}\n",
    "        \n",
    "        response = make_s2_request(f\"paper/batch\", {**params, \"ids\": ids_param})\n",
    "        \n",
    "        if response:\n",
    "            for paper in response:\n",
    "                if paper:  # Some papers might not be found\n",
    "                    paper_data = {\n",
    "                        \"paper_id\": paper.get(\"paperId\"),\n",
    "                        \"title\": paper.get(\"title\"),\n",
    "                        \"year\": paper.get(\"year\"),\n",
    "                        \"venue\": paper.get(\"venue\"),\n",
    "                        \"citation_count\": paper.get(\"citationCount\", 0),\n",
    "                        \"author_count\": len(paper.get(\"authors\", [])),\n",
    "                        \"authors\": [auth.get(\"name\") for auth in paper.get(\"authors\", [])]\n",
    "                    }\n",
    "                    papers_data.append(paper_data)\n",
    "        \n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(paper_ids) + batch_size - 1)//batch_size}\")\n",
    "    \n",
    "    return pd.DataFrame(papers_data)\n",
    "\n",
    "def find_influential_papers(query: str, min_influential_citations: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find highly influential papers based on influential citation count.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        min_influential_citations: Minimum influential citation count\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with influential papers\n",
    "    \"\"\"\n",
    "    # Search for papers\n",
    "    papers = search_papers(query, limit=200)\n",
    "    \n",
    "    if papers.empty:\n",
    "        return papers\n",
    "    \n",
    "    # Filter by influential citation count\n",
    "    influential = papers[papers[\"influential_citation_count\"] >= min_influential_citations]\n",
    "    \n",
    "    # Sort by influential citation count\n",
    "    influential = influential.sort_values(\"influential_citation_count\", ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(influential)} influential papers (≥{min_influential_citations} influential citations)\")\n",
    "    \n",
    "    return influential\n",
    "\n",
    "def analyze_research_trends(papers_df: pd.DataFrame, time_window: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze research trends from papers DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame containing papers with year information\n",
    "        time_window: Years to consider for recent trend analysis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trend analysis\n",
    "    \"\"\"\n",
    "    if papers_df.empty or \"year\" not in papers_df.columns:\n",
    "        return {}\n",
    "    \n",
    "    # Filter valid years\n",
    "    valid_papers = papers_df[papers_df[\"year\"].notna() & (papers_df[\"year\"] > 1950)]\n",
    "    \n",
    "    if valid_papers.empty:\n",
    "        return {}\n",
    "    \n",
    "    current_year = 2024  # or datetime.now().year\n",
    "    recent_cutoff = current_year - time_window\n",
    "    \n",
    "    # Temporal trends\n",
    "    yearly_counts = valid_papers.groupby(\"year\").size().to_dict()\n",
    "    recent_papers = valid_papers[valid_papers[\"year\"] >= recent_cutoff]\n",
    "    \n",
    "    # CitedBy trends\n",
    "    citation_stats = {\n",
    "        \"total_papers\": len(valid_papers),\n",
    "        \"recent_papers\": len(recent_papers),\n",
    "        \"avg_citations_all\": valid_papers[\"citation_count\"].mean(),\n",
    "        \"avg_citations_recent\": recent_papers[\"citation_count\"].mean() if not recent_papers.empty else 0,\n",
    "        \"yearly_publication_count\": yearly_counts,\n",
    "        \"top_cited_recent\": recent_papers.nlargest(5, \"citation_count\")[[\"title\", \"year\", \"citation_count\"]].to_dict(\"records\") if not recent_papers.empty else []\n",
    "    }\n",
    "    \n",
    "    # Venue analysis\n",
    "    if \"venue\" in valid_papers.columns:\n",
    "        venue_counts = valid_papers[\"venue\"].value_counts().head(10).to_dict()\n",
    "        citation_stats[\"top_venues\"] = venue_counts\n",
    "    \n",
    "    return citation_stats\n",
    "\n",
    "# Example: Get recommendations for a transformer paper\n",
    "if 'transformer_papers' in locals() and len(transformer_papers) > 0:\n",
    "    sample_paper = transformer_papers.iloc[0]\n",
    "    print(f\"\\nExample 4: Getting recommendations for '{sample_paper['title']}'\")\n",
    "    \n",
    "    recommendations = get_recommendations(sample_paper[\"paper_id\"], limit=10)\n",
    "    \n",
    "    if not recommendations.empty:\n",
    "        print(f\"\\nTop 5 recommendations:\")\n",
    "        print(recommendations[[\"title\", \"year\", \"citation_count\", \"venue\"]].head())\n",
    "        \n",
    "        # Analyze trends in transformer research\n",
    "        print(f\"\\nTransformer Research Trends:\")\n",
    "        trends = analyze_research_trends(transformer_papers)\n",
    "        print(f\"- Total papers: {trends.get('total_papers', 0)}\")\n",
    "        print(f\"- Recent papers (last 5 years): {trends.get('recent_papers', 0)}\")\n",
    "        print(f\"- Average citations: {trends.get('avg_citations_all', 0):.1f}\")\n",
    "else:\n",
    "    print(\"No transformer papers available for recommendations. Run the paper search cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dda070",
   "metadata": {},
   "source": [
    "## 8. Advanced Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_semantic_scholar_data(papers_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Advanced processing of Semantic Scholar paper data.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: Raw papers DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Processed and enriched DataFrame\n",
    "    \"\"\"\n",
    "    df = papers_df.copy()\n",
    "    \n",
    "    print(f\"Processing {len(df)} papers...\")\n",
    "    \n",
    "    # Data cleaning\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"\").str.strip()\n",
    "    df[\"abstract\"] = df[\"abstract\"].fillna(\"\")\n",
    "    \n",
    "    # Year validation\n",
    "    current_year = 2024\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    df[\"year_valid\"] = (df[\"year\"] >= 1950) & (df[\"year\"] <= current_year)\n",
    "    \n",
    "    # CitedBy metrics\n",
    "    df[\"citation_count\"] = pd.to_numeric(df[\"citation_count\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"influential_citation_count\"] = pd.to_numeric(df[\"influential_citation_count\"], errors=\"coerce\").fillna(0)\n",
    "    \n",
    "    # Calculate influence ratio (what % of citations are influential)\n",
    "    df[\"influence_ratio\"] = np.where(\n",
    "        df[\"citation_count\"] > 0,\n",
    "        df[\"influential_citation_count\"] / df[\"citation_count\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Recency scoring (higher score for recent papers)\n",
    "    df[\"recency_score\"] = np.where(\n",
    "        df[\"year_valid\"],\n",
    "        (df[\"year\"] - 1950) / (current_year - 1950),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Impact score combining citations and recency\n",
    "    max_citations = df[\"citation_count\"].max() if len(df) > 0 else 1\n",
    "    df[\"normalized_citations\"] = df[\"citation_count\"] / max_citations if max_citations > 0 else 0\n",
    "    df[\"impact_score\"] = (0.7 * df[\"normalized_citations\"] + 0.3 * df[\"recency_score\"])\n",
    "    \n",
    "    # Text processing\n",
    "    df[\"title_length\"] = df[\"title\"].str.len()\n",
    "    df[\"has_abstract\"] = df[\"abstract\"].str.len() > 50\n",
    "    df[\"abstract_length\"] = df[\"abstract\"].str.len()\n",
    "    \n",
    "    # Author processing\n",
    "    df[\"author_count\"] = df[\"author_count\"].fillna(0).astype(int)\n",
    "    df[\"is_single_author\"] = df[\"author_count\"] == 1\n",
    "    df[\"is_collaboration\"] = df[\"author_count\"] > 3\n",
    "    \n",
    "    # Field of study processing\n",
    "    if \"fields_of_study\" in df.columns:\n",
    "        df[\"field_count\"] = df[\"fields_of_study\"].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        df[\"is_interdisciplinary\"] = df[\"field_count\"] > 2\n",
    "    \n",
    "    # Venue processing\n",
    "    df[\"has_venue\"] = df[\"venue\"].notna() & (df[\"venue\"] != \"\")\n",
    "    \n",
    "    print(f\"Processing completed. Added {len([c for c in df.columns if c not in papers_df.columns])} new features.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_keywords_from_abstracts(abstracts: List[str], min_length: int = 4, \n",
    "                                  top_n: int = 50) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extract keywords from paper abstracts.\n",
    "    \n",
    "    Args:\n",
    "        abstracts: List of abstract strings\n",
    "        min_length: Minimum word length to consider\n",
    "        top_n: Number of top keywords to return\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of keywords and their frequencies\n",
    "    \"\"\"\n",
    "    # Combine all abstracts\n",
    "    text = \" \".join([abs for abs in abstracts if abs and len(abs) > 50])\n",
    "    \n",
    "    if not text:\n",
    "        return {}\n",
    "    \n",
    "    # Simple keyword extraction (could be enhanced with NLP libraries)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove common academic stopwords\n",
    "    stopwords = {\n",
    "        \"the\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\", \n",
    "        \"by\", \"from\", \"this\", \"that\", \"these\", \"those\", \"we\", \"our\", \"they\",\n",
    "        \"paper\", \"study\", \"research\", \"method\", \"approach\", \"results\", \"conclusion\",\n",
    "        \"analysis\", \"data\", \"using\", \"based\", \"show\", \"shows\", \"present\", \"propose\"\n",
    "    }\n",
    "    \n",
    "    # Extract words\n",
    "    words = re.findall(r'\\b[a-zA-Z]{' + str(min_length) + ',}\\b', text)\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    \n",
    "    # Count frequencies\n",
    "    keyword_counts = Counter(words)\n",
    "    \n",
    "    return dict(keyword_counts.most_common(top_n))\n",
    "\n",
    "def create_citation_network(papers_df: pd.DataFrame, citations_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a citation network from papers and citations data.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame with paper information\n",
    "        citations_df: DataFrame with citation relationships\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame representing citation network edges\n",
    "    \"\"\"\n",
    "    network_edges = []\n",
    "    \n",
    "    # Create paper ID to title mapping\n",
    "    id_to_title = dict(zip(papers_df[\"paper_id\"], papers_df[\"title\"]))\n",
    "    \n",
    "    for _, citation in citations_df.iterrows():\n",
    "        citing_id = citation.get(\"citing_paper_id\")\n",
    "        cited_id = papers_df.iloc[0][\"paper_id\"] if len(papers_df) > 0 else None  # Assuming we're analyzing citations to papers in our dataset\n",
    "        \n",
    "        if citing_id and cited_id:\n",
    "            edge = {\n",
    "                \"source\": cited_id,\n",
    "                \"target\": citing_id,\n",
    "                \"source_title\": id_to_title.get(cited_id, \"Unknown\"),\n",
    "                \"target_title\": citation.get(\"citing_title\", \"Unknown\"),\n",
    "                \"citation_year\": citation.get(\"citing_year\"),\n",
    "                \"context_count\": citation.get(\"context_count\", 0),\n",
    "                \"weight\": 1 + citation.get(\"context_count\", 0)  # Weight by context richness\n",
    "            }\n",
    "            network_edges.append(edge)\n",
    "    \n",
    "    return pd.DataFrame(network_edges)\n",
    "\n",
    "def generate_research_summary(papers_df: pd.DataFrame, field_name: str = \"research area\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of research data.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: DataFrame with processed paper data\n",
    "        field_name: Name of the research field being analyzed\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with comprehensive research summary\n",
    "    \"\"\"\n",
    "    if papers_df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    total_papers = len(papers_df)\n",
    "    year_range = (papers_df[\"year\"].min(), papers_df[\"year\"].max()) if \"year\" in papers_df.columns else None\n",
    "    \n",
    "    # CitedBy analysis\n",
    "    citation_stats = {\n",
    "        \"total_citations\": papers_df[\"citation_count\"].sum(),\n",
    "        \"avg_citations\": papers_df[\"citation_count\"].mean(),\n",
    "        \"median_citations\": papers_df[\"citation_count\"].median(),\n",
    "        \"max_citations\": papers_df[\"citation_count\"].max(),\n",
    "        \"papers_with_citations\": (papers_df[\"citation_count\"] > 0).sum(),\n",
    "    }\n",
    "    \n",
    "    # Influence analysis\n",
    "    if \"influential_citation_count\" in papers_df.columns:\n",
    "        influence_stats = {\n",
    "            \"total_influential_citations\": papers_df[\"influential_citation_count\"].sum(),\n",
    "            \"avg_influence_ratio\": papers_df[\"influence_ratio\"].mean() if \"influence_ratio\" in papers_df.columns else 0,\n",
    "            \"highly_influential_papers\": (papers_df[\"influential_citation_count\"] >= 10).sum()\n",
    "        }\n",
    "    else:\n",
    "        influence_stats = {}\n",
    "    \n",
    "    # Temporal analysis\n",
    "    if \"year\" in papers_df.columns:\n",
    "        recent_papers = papers_df[papers_df[\"year\"] >= 2020] if papers_df[\"year\"].max() >= 2020 else pd.DataFrame()\n",
    "        temporal_stats = {\n",
    "            \"recent_papers_count\": len(recent_papers),\n",
    "            \"recent_growth_rate\": len(recent_papers) / total_papers if total_papers > 0 else 0,\n",
    "            \"peak_year\": papers_df[\"year\"].value_counts().idxmax() if \"year\" in papers_df.columns else None\n",
    "        }\n",
    "    else:\n",
    "        temporal_stats = {}\n",
    "    \n",
    "    # Collaboration analysis\n",
    "    if \"author_count\" in papers_df.columns:\n",
    "        collab_stats = {\n",
    "            \"avg_authors_per_paper\": papers_df[\"author_count\"].mean(),\n",
    "            \"single_author_papers\": (papers_df[\"author_count\"] == 1).sum(),\n",
    "            \"large_collaborations\": (papers_df[\"author_count\"] >= 5).sum(),\n",
    "            \"collaboration_rate\": ((papers_df[\"author_count\"] > 1).sum() / total_papers) if total_papers > 0 else 0\n",
    "        }\n",
    "    else:\n",
    "        collab_stats = {}\n",
    "    \n",
    "    # Top papers\n",
    "    top_papers = papers_df.nlargest(5, \"citation_count\")[[\"title\", \"year\", \"citation_count\"]].to_dict(\"records\") if not papers_df.empty else []\n",
    "    \n",
    "    # Venue analysis\n",
    "    venue_stats = {}\n",
    "    if \"venue\" in papers_df.columns:\n",
    "        top_venues = papers_df[\"venue\"].value_counts().head(10).to_dict()\n",
    "        venue_stats = {\"top_venues\": top_venues}\n",
    "    \n",
    "    summary = {\n",
    "        \"field_name\": field_name,\n",
    "        \"overview\": {\n",
    "            \"total_papers\": total_papers,\n",
    "            \"year_range\": year_range,\n",
    "            **citation_stats\n",
    "        },\n",
    "        \"influence_metrics\": influence_stats,\n",
    "        \"temporal_trends\": temporal_stats,\n",
    "        \"collaboration_patterns\": collab_stats,\n",
    "        \"venue_distribution\": venue_stats,\n",
    "        \"top_papers\": top_papers,\n",
    "        \"generated_at\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example: Process transformer papers data\n",
    "if 'transformer_papers' in locals() and len(transformer_papers) > 0:\n",
    "    print(\"Example 5: Advanced data processing...\")\n",
    "    \n",
    "    # Process the data\n",
    "    processed_papers = process_semantic_scholar_data(transformer_papers)\n",
    "    \n",
    "    print(f\"\\nProcessed paper statistics:\")\n",
    "    print(f\"- Papers with valid years: {processed_papers['year_valid'].sum()}\")\n",
    "    print(f\"- Papers with abstracts: {processed_papers['has_abstract'].sum()}\")\n",
    "    print(f\"- Average impact score: {processed_papers['impact_score'].mean():.3f}\")\n",
    "    print(f\"- Collaboration rate: {(processed_papers['author_count'] > 1).mean():.1%}\")\n",
    "    \n",
    "    # Extract keywords from abstracts\n",
    "    abstracts = processed_papers[\"abstract\"].tolist()\n",
    "    keywords = extract_keywords_from_abstracts(abstracts, top_n=20)\n",
    "    print(f\"\\nTop keywords in abstracts:\")\n",
    "    for keyword, count in list(keywords.items())[:10]:\n",
    "        print(f\"- {keyword}: {count}\")\n",
    "    \n",
    "    # Generate research summary\n",
    "    summary = generate_research_summary(processed_papers, \"Transformer Research\")\n",
    "    print(f\"\\nResearch Summary:\")\n",
    "    print(f\"- Total papers: {summary['overview']['total_papers']}\")\n",
    "    print(f\"- CitedBy range: {summary['overview']['year_range']}\")\n",
    "    print(f\"- Average citations: {summary['overview']['avg_citations']:.1f}\")\n",
    "else:\n",
    "    print(\"No transformer papers available for processing. Run the paper search cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b3a3d",
   "metadata": {},
   "source": [
    "## 9. Data Export and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_semantic_scholar_data(datasets: Dict[str, pd.DataFrame], \n",
    "                                base_name: str = \"semantic_scholar\") -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Export multiple DataFrames to various formats.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping dataset names to DataFrames\n",
    "        base_name: Base filename for exports\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset names to exported file paths\n",
    "    \"\"\"\n",
    "    # Create data directory\n",
    "    data_dir = \"/home/dnhoa/IAAIR/data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    exported_files = {}\n",
    "    \n",
    "    for dataset_name, df in datasets.items():\n",
    "        if df.empty:\n",
    "            print(f\"Skipping empty dataset: {dataset_name}\")\n",
    "            continue\n",
    "            \n",
    "        filename_base = f\"{base_name}_{dataset_name}_{timestamp}\"\n",
    "        dataset_files = {}\n",
    "        \n",
    "        # Export to CSV\n",
    "        csv_path = os.path.join(data_dir, f\"{filename_base}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        dataset_files[\"csv\"] = csv_path\n",
    "        print(f\"✅ Exported {dataset_name} to CSV: {csv_path}\")\n",
    "        \n",
    "        # Export to JSON\n",
    "        json_path = os.path.join(data_dir, f\"{filename_base}.json\")\n",
    "        df.to_json(json_path, orient=\"records\", indent=2)\n",
    "        dataset_files[\"json\"] = json_path\n",
    "        print(f\"✅ Exported {dataset_name} to JSON: {json_path}\")\n",
    "        \n",
    "        # Export to Excel (if possible)\n",
    "        try:\n",
    "            excel_path = os.path.join(data_dir, f\"{filename_base}.xlsx\")\n",
    "            df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "            dataset_files[\"excel\"] = excel_path\n",
    "            print(f\"✅ Exported {dataset_name} to Excel: {excel_path}\")\n",
    "        except ImportError:\n",
    "            print(f\"⚠️  Excel export skipped for {dataset_name} (install openpyxl)\")\n",
    "        \n",
    "        exported_files[dataset_name] = dataset_files\n",
    "    \n",
    "    return exported_files\n",
    "\n",
    "def create_knowledge_fabric_schema(papers_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create schema mapping for Knowledge Fabric integration.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: Processed papers DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Schema dictionary for Knowledge Fabric\n",
    "    \"\"\"\n",
    "    # Map Semantic Scholar fields to Knowledge Fabric schema\n",
    "    schema = {\n",
    "        \"data_source\": \"semantic_scholar\",\n",
    "        \"entity_types\": {\n",
    "            \"documents\": {\n",
    "                \"id_field\": \"paper_id\",\n",
    "                \"title_field\": \"title\",\n",
    "                \"abstract_field\": \"abstract\",\n",
    "                \"year_field\": \"year\",\n",
    "                \"citation_count_field\": \"citation_count\",\n",
    "                \"authors_field\": \"authors\",\n",
    "                \"venue_field\": \"venue\"\n",
    "            },\n",
    "            \"authors\": {\n",
    "                \"id_field\": \"author_id\",\n",
    "                \"name_field\": \"name\",\n",
    "                \"paper_count_field\": \"paper_count\",\n",
    "                \"citation_count_field\": \"citation_count\",\n",
    "                \"h_index_field\": \"h_index\"\n",
    "            }\n",
    "        },\n",
    "        \"relationships\": {\n",
    "            \"authorship\": {\n",
    "                \"source\": \"authors\",\n",
    "                \"target\": \"documents\",\n",
    "                \"properties\": [\"position\", \"is_corresponding\"]\n",
    "            },\n",
    "            \"citation\": {\n",
    "                \"source\": \"documents\",\n",
    "                \"target\": \"documents\", \n",
    "                \"properties\": [\"citation_contexts\", \"influential\"]\n",
    "            }\n",
    "        },\n",
    "        \"field_mappings\": {\n",
    "            # Map S2 fields to standard names\n",
    "            \"paperId\": \"document_id\",\n",
    "            \"authorId\": \"author_id\",\n",
    "            \"citationCount\": \"citation_count\",\n",
    "            \"influentialCitationCount\": \"influential_citation_count\",\n",
    "            \"isOpenAccess\": \"is_open_access\"\n",
    "        },\n",
    "        \"sample_data\": papers_df.head(3).to_dict(\"records\") if not papers_df.empty else []\n",
    "    }\n",
    "    \n",
    "    return schema\n",
    "\n",
    "def export_for_knowledge_fabric(papers_df: pd.DataFrame, authors_df: pd.DataFrame = None,\n",
    "                               citations_df: pd.DataFrame = None) -> str:\n",
    "    \"\"\"\n",
    "    Export data in Knowledge Fabric compatible format.\n",
    "    \n",
    "    Args:\n",
    "        papers_df: Papers DataFrame\n",
    "        authors_df: Authors DataFrame (optional)\n",
    "        citations_df: Citations DataFrame (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Path to exported JSON file\n",
    "    \"\"\"\n",
    "    data_dir = \"/home/dnhoa/IAAIR/data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create Knowledge Fabric compatible structure\n",
    "    kg_data = {\n",
    "        \"metadata\": {\n",
    "            \"source\": \"semantic_scholar\",\n",
    "            \"exported_at\": datetime.now().isoformat(),\n",
    "            \"schema_version\": \"1.0\",\n",
    "            \"total_papers\": len(papers_df) if not papers_df.empty else 0,\n",
    "            \"total_authors\": len(authors_df) if authors_df is not None and not authors_df.empty else 0,\n",
    "            \"total_citations\": len(citations_df) if citations_df is not None and not citations_df.empty else 0\n",
    "        },\n",
    "        \"documents\": papers_df.to_dict(\"records\") if not papers_df.empty else [],\n",
    "        \"authors\": authors_df.to_dict(\"records\") if authors_df is not None and not authors_df.empty else [],\n",
    "        \"citations\": citations_df.to_dict(\"records\") if citations_df is not None and not citations_df.empty else [],\n",
    "        \"schema\": create_knowledge_fabric_schema(papers_df)\n",
    "    }\n",
    "    \n",
    "    # Export to JSON\n",
    "    kg_file = os.path.join(data_dir, f\"knowledge_fabric_import_{timestamp}.json\")\n",
    "    with open(kg_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(kg_data, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"✅ Exported Knowledge Fabric data: {kg_file}\")\n",
    "    print(f\"   - Papers: {len(kg_data['documents'])}\")\n",
    "    print(f\"   - Authors: {len(kg_data['authors'])}\")\n",
    "    print(f\"   - Citations: {len(kg_data['citations'])}\")\n",
    "    \n",
    "    return kg_file\n",
    "\n",
    "def create_data_summary_report(exported_files: Dict[str, Dict[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary report of all exported data.\n",
    "    \n",
    "    Args:\n",
    "        exported_files: Dictionary of exported file paths\n",
    "        \n",
    "    Returns:\n",
    "        Summary DataFrame\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for dataset_name, files in exported_files.items():\n",
    "        for format_type, filepath in files.items():\n",
    "            # Get file size\n",
    "            file_size = os.path.getsize(filepath) if os.path.exists(filepath) else 0\n",
    "            file_size_mb = round(file_size / (1024 * 1024), 2)\n",
    "            \n",
    "            summary = {\n",
    "                \"dataset\": dataset_name,\n",
    "                \"format\": format_type,\n",
    "                \"filepath\": filepath,\n",
    "                \"size_mb\": file_size_mb,\n",
    "                \"exported_at\": datetime.now().isoformat()\n",
    "            }\n",
    "            summary_data.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Example: Export all collected data\n",
    "datasets_to_export = {}\n",
    "\n",
    "# Collect available datasets\n",
    "if 'transformer_papers' in locals():\n",
    "    datasets_to_export['transformer_papers'] = transformer_papers\n",
    "if 'processed_papers' in locals():\n",
    "    datasets_to_export['processed_papers'] = processed_papers\n",
    "if 'ai_researchers' in locals():\n",
    "    datasets_to_export['ai_researchers'] = ai_researchers\n",
    "if 'citations' in locals():\n",
    "    datasets_to_export['citations'] = citations\n",
    "if 'recommendations' in locals():\n",
    "    datasets_to_export['recommendations'] = recommendations\n",
    "\n",
    "if datasets_to_export:\n",
    "    print(\"Exporting Semantic Scholar datasets...\")\n",
    "    \n",
    "    # Export to multiple formats\n",
    "    exported_files = export_semantic_scholar_data(datasets_to_export)\n",
    "    \n",
    "    # Export for Knowledge Fabric\n",
    "    papers_data = datasets_to_export.get('processed_papers', datasets_to_export.get('transformer_papers'))\n",
    "    authors_data = datasets_to_export.get('ai_researchers')\n",
    "    citations_data = datasets_to_export.get('citations')\n",
    "    \n",
    "    if papers_data is not None and not papers_data.empty:\n",
    "        kg_file = export_for_knowledge_fabric(papers_data, authors_data, citations_data)\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_report = create_data_summary_report(exported_files)\n",
    "    \n",
    "    print(f\"\\n=== Export Summary ===\")\n",
    "    print(summary_report.to_string(index=False))\n",
    "    \n",
    "    # Export summary report\n",
    "    summary_path = os.path.join(\"/home/dnhoa/IAAIR/data\", f\"export_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\")\n",
    "    summary_report.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nExport summary saved: {summary_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No datasets available for export. Run the data collection cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52734529",
   "metadata": {},
   "source": [
    "## 10. Visualization and Analysis\n",
    "\n",
    "Quick visualizations to understand the data patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab15119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualizations(papers_df: pd.DataFrame):\n",
    "    \"\"\"Create basic visualizations for paper data analysis\"\"\"\n",
    "    \n",
    "    if papers_df.empty:\n",
    "        print(\"No data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Set up the plotting style\n",
    "        plt.style.use('default')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Semantic Scholar Data Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Papers by year\n",
    "        if 'year' in papers_df.columns:\n",
    "            valid_years = papers_df[papers_df['year'].notna() & (papers_df['year'] >= 1990)]\n",
    "            if not valid_years.empty:\n",
    "                yearly_counts = valid_years['year'].value_counts().sort_index()\n",
    "                axes[0, 0].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, markersize=4)\n",
    "                axes[0, 0].set_title('Publications by Year', fontweight='bold')\n",
    "                axes[0, 0].set_xlabel('Year')\n",
    "                axes[0, 0].set_ylabel('Number of Papers')\n",
    "                axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. CitedBy distribution\n",
    "        if 'citation_count' in papers_df.columns:\n",
    "            citations = papers_df['citation_count'][papers_df['citation_count'] >= 0]\n",
    "            if not citations.empty:\n",
    "                # Use log scale for better visualization\n",
    "                log_citations = np.log1p(citations)  # log(1 + x) to handle 0 citations\n",
    "                axes[0, 1].hist(log_citations, bins=30, alpha=0.7, edgecolor='black')\n",
    "                axes[0, 1].set_title('CitedBy Distribution (Log Scale)', fontweight='bold')\n",
    "                axes[0, 1].set_xlabel('Log(1 + Citations)')\n",
    "                axes[0, 1].set_ylabel('Number of Papers')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Collaboration patterns\n",
    "        if 'author_count' in papers_df.columns:\n",
    "            author_counts = papers_df['author_count'][papers_df['author_count'] > 0]\n",
    "            if not author_counts.empty:\n",
    "                collaboration_dist = author_counts.value_counts().sort_index()[:10]  # Top 10 author counts\n",
    "                axes[1, 0].bar(collaboration_dist.index, collaboration_dist.values, alpha=0.7, edgecolor='black')\n",
    "                axes[1, 0].set_title('Collaboration Patterns', fontweight='bold')\n",
    "                axes[1, 0].set_xlabel('Number of Authors')\n",
    "                axes[1, 0].set_ylabel('Number of Papers')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Top venues\n",
    "        if 'venue' in papers_df.columns:\n",
    "            venues = papers_df['venue'].dropna()\n",
    "            if not venues.empty and len(venues) > 0:\n",
    "                top_venues = venues.value_counts().head(10)\n",
    "                axes[1, 1].barh(range(len(top_venues)), top_venues.values, alpha=0.7, edgecolor='black')\n",
    "                axes[1, 1].set_yticks(range(len(top_venues)))\n",
    "                axes[1, 1].set_yticklabels([v[:30] + '...' if len(v) > 30 else v for v in top_venues.index], fontsize=9)\n",
    "                axes[1, 1].set_title('Top Publication Venues', fontweight='bold')\n",
    "                axes[1, 1].set_xlabel('Number of Papers')\n",
    "                axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "        print(\"Note: Make sure matplotlib and seaborn are installed for visualizations\")\n",
    "\n",
    "def print_data_insights(papers_df: pd.DataFrame):\n",
    "    \"\"\"Print key insights from the data\"\"\"\n",
    "    \n",
    "    if papers_df.empty:\n",
    "        print(\"No data available for insights\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== Key Data Insights ===\\n\")\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"📊 Dataset Overview:\")\n",
    "    print(f\"   Total papers: {len(papers_df):,}\")\n",
    "    \n",
    "    if 'year' in papers_df.columns:\n",
    "        valid_years = papers_df[papers_df['year'].notna()]\n",
    "        if not valid_years.empty:\n",
    "            print(f\"   Year range: {valid_years['year'].min():.0f} - {valid_years['year'].max():.0f}\")\n",
    "            recent_papers = valid_years[valid_years['year'] >= 2020]\n",
    "            print(f\"   Recent papers (2020+): {len(recent_papers):,} ({len(recent_papers)/len(valid_years):.1%})\")\n",
    "    \n",
    "    # CitedBy insights\n",
    "    if 'citation_count' in papers_df.columns:\n",
    "        citations = papers_df['citation_count'][papers_df['citation_count'] >= 0]\n",
    "        if not citations.empty:\n",
    "            print(f\"\\n📈 CitedBy Metrics:\")\n",
    "            print(f\"   Total citations: {citations.sum():,}\")\n",
    "            print(f\"   Average citations: {citations.mean():.1f}\")\n",
    "            print(f\"   Median citations: {citations.median():.1f}\")\n",
    "            print(f\"   Most cited paper: {citations.max():,} citations\")\n",
    "            highly_cited = citations[citations >= 100]\n",
    "            print(f\"   Highly cited papers (100+ citations): {len(highly_cited):,} ({len(highly_cited)/len(citations):.1%})\")\n",
    "    \n",
    "    # Collaboration insights\n",
    "    if 'author_count' in papers_df.columns:\n",
    "        authors = papers_df['author_count'][papers_df['author_count'] > 0]\n",
    "        if not authors.empty:\n",
    "            print(f\"\\n🤝 Collaboration Patterns:\")\n",
    "            print(f\"   Average authors per paper: {authors.mean():.1f}\")\n",
    "            single_author = authors[authors == 1]\n",
    "            print(f\"   Single-author papers: {len(single_author):,} ({len(single_author)/len(authors):.1%})\")\n",
    "            large_collab = authors[authors >= 5]\n",
    "            print(f\"   Large collaborations (5+ authors): {len(large_collab):,} ({len(large_collab)/len(authors):.1%})\")\n",
    "    \n",
    "    # Open access insights\n",
    "    if 'is_open_access' in papers_df.columns:\n",
    "        oa_data = papers_df['is_open_access']\n",
    "        oa_count = oa_data.sum() if oa_data.dtype == bool else (oa_data == True).sum()\n",
    "        print(f\"\\n🔓 Open Access:\")\n",
    "        print(f\"   Open access papers: {oa_count:,} ({oa_count/len(papers_df):.1%})\")\n",
    "    \n",
    "    # Top papers\n",
    "    if 'citation_count' in papers_df.columns and 'title' in papers_df.columns:\n",
    "        top_papers = papers_df.nlargest(3, 'citation_count')\n",
    "        print(f\"\\n🏆 Most CitedBy Papers:\")\n",
    "        for i, (_, paper) in enumerate(top_papers.iterrows(), 1):\n",
    "            title = paper['title'][:60] + \"...\" if len(str(paper['title'])) > 60 else paper['title']\n",
    "            citations = paper['citation_count']\n",
    "            year = f\" ({paper['year']:.0f})\" if 'year' in papers_df.columns and pd.notna(paper['year']) else \"\"\n",
    "            print(f\"   {i}. {title}{year} - {citations:,} citations\")\n",
    "\n",
    "# Generate visualizations and insights if we have data\n",
    "if 'transformer_papers' in locals() and not transformer_papers.empty:\n",
    "    print(\"Example 6: Data Visualization and Insights\")\n",
    "    print_data_insights(transformer_papers)\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    create_visualizations(transformer_papers)\n",
    "elif 'processed_papers' in locals() and not processed_papers.empty:\n",
    "    print(\"Example 6: Data Visualization and Insights\")\n",
    "    print_data_insights(processed_papers)\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    create_visualizations(processed_papers)\n",
    "else:\n",
    "    print(\"No paper data available for visualization. Run the data collection cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b951d2a",
   "metadata": {},
   "source": [
    "## 11. Next Steps and Integration\n",
    "\n",
    "### Key Features of Semantic Scholar API:\n",
    "- **Rich CitedBy Data**: Full citation contexts and influential citation metrics\n",
    "- **AI-Powered Recommendations**: Advanced paper recommendation system\n",
    "- **Author Networks**: Comprehensive author profiles and collaboration data\n",
    "- **Embeddings**: Paper embeddings for semantic similarity\n",
    "- **TLDR Generation**: AI-generated paper summaries\n",
    "\n",
    "### Integration with Knowledge Fabric:\n",
    "1. **Document Ingestion**: Use the processed papers data for document entities\n",
    "2. **Author Networks**: Leverage author collaboration data for network analysis\n",
    "3. **CitedBy Graphs**: Build citation networks with context information\n",
    "4. **Semantic Search**: Use paper embeddings for similarity search\n",
    "5. **Attribution**: Use citation contexts for evidence attribution\n",
    "\n",
    "### Best Practices:\n",
    "1. **API Key**: Get an API key for higher rate limits (1000 req/5min)\n",
    "2. **Batch Processing**: Use batch endpoints for efficiency\n",
    "3. **CitedBy Context**: Leverage rich citation context data\n",
    "4. **Influential Metrics**: Use influential citation counts for impact assessment\n",
    "5. **Data Quality**: Filter and validate data before processing\n",
    "\n",
    "### Complementary to OpenAlex:\n",
    "- **OpenAlex**: Broader coverage, completely open data\n",
    "- **Semantic Scholar**: Richer features, AI-powered insights, citation contexts\n",
    "- **Combined Use**: Use both APIs for comprehensive coverage\n",
    "\n",
    "### Resources:\n",
    "- [Semantic Scholar API Documentation](https://api.semanticscholar.org/)\n",
    "- [Academic Graph Dataset](https://www.semanticscholar.org/product/api#academic-graph-api)\n",
    "- [API Key Registration](https://www.semanticscholar.org/product/api#api-key)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
