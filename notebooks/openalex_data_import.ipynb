{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fe5f03",
   "metadata": {},
   "source": [
    "# OpenAlex Data Import Notebook\n",
    "\n",
    "This notebook demonstrates how to import and work with data from the OpenAlex API. OpenAlex is a comprehensive, open database of academic works, authors, institutions, concepts, and more.\n",
    "\n",
    "## OpenAlex API Overview\n",
    "- **Base URL**: https://api.openalex.org/\n",
    "- **Open Access**: No authentication required for basic usage\n",
    "- **Rate Limiting**: Polite usage recommended (1 request per second)\n",
    "- **Coverage**: 240M+ works, 90M+ authors, 100K+ institutions\n",
    "\n",
    "## What we'll cover:\n",
    "1. Setting up API connections\n",
    "2. Fetching different types of data (works, authors, institutions)\n",
    "3. Handling pagination for large datasets\n",
    "4. Processing and cleaning data\n",
    "5. Exporting data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9925a4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for API interaction and data handling\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Any\n",
    "from urllib.parse import urljoin, urlencode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For data visualization (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    print(\"Visualization libraries loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"Visualization libraries not available - install matplotlib and seaborn for plots\")\n",
    "\n",
    "print(\"All required libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef0ec4",
   "metadata": {},
   "source": [
    "## 2. Set Up API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAlex API Configuration\n",
    "class OpenAlexConfig:\n",
    "    \"\"\"Configuration class for OpenAlex API\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.openalex.org\"\n",
    "    \n",
    "    # Common headers - add your email for higher rate limits\n",
    "    HEADERS = {\n",
    "        \"User-Agent\": \"KnowledgeFabric/1.0 (mailto:your-email@domain.com)\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Rate limiting - be polite to the API\n",
    "    REQUEST_DELAY = 1.1  # seconds between requests (slightly above 1/sec)\n",
    "    \n",
    "    # Common parameters\n",
    "    DEFAULT_PARAMS = {\n",
    "        \"per-page\": 25,  # results per page (max 200)\n",
    "        \"mailto\": \"your-email@domain.com\"  # for polite pool access\n",
    "    }\n",
    "\n",
    "# Initialize configuration\n",
    "config = OpenAlexConfig()\n",
    "print(f\"OpenAlex API Base URL: {config.BASE_URL}\")\n",
    "print(f\"Default results per page: {config.DEFAULT_PARAMS['per-page']}\")\n",
    "print(f\"Request delay: {config.REQUEST_DELAY} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedd637",
   "metadata": {},
   "source": [
    "## 3. Basic API Request Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_openalex_request(endpoint: str, params: Dict = None, delay: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    Make a request to the OpenAlex API with error handling and rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        endpoint: API endpoint (e.g., 'works', 'authors', 'institutions')\n",
    "        params: Query parameters\n",
    "        delay: Whether to add delay for rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        JSON response as dictionary\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    # Merge with default parameters\n",
    "    final_params = {**config.DEFAULT_PARAMS, **params}\n",
    "    \n",
    "    # Build URL\n",
    "    url = urljoin(config.BASE_URL, endpoint)\n",
    "    \n",
    "    try:\n",
    "        # Rate limiting\n",
    "        if delay:\n",
    "            time.sleep(config.REQUEST_DELAY)\n",
    "        \n",
    "        # Make request\n",
    "        response = requests.get(url, headers=config.HEADERS, params=final_params)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error making request to {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_api_connection():\n",
    "    \"\"\"Test the API connection with a simple request\"\"\"\n",
    "    print(\"Testing OpenAlex API connection...\")\n",
    "    \n",
    "    response = make_openalex_request(\"works\", {\"filter\": \"publication_year:2023\", \"per-page\": 1})\n",
    "    \n",
    "    if response and \"results\" in response:\n",
    "        print(\"✅ API connection successful!\")\n",
    "        print(f\"Total works in 2023: {response['meta']['count']:,}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ API connection failed!\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "test_api_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e878a7",
   "metadata": {},
   "source": [
    "## 4. Fetch Works Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ea9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_works(filters: Dict = None, limit: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch academic works from OpenAlex API.\n",
    "    \n",
    "    Args:\n",
    "        filters: Dictionary of filters to apply\n",
    "        limit: Maximum number of results to fetch\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with works data\n",
    "    \"\"\"\n",
    "    works_data = []\n",
    "    per_page = min(25, limit)  # API max is 200, but start with 25\n",
    "    pages_needed = (limit + per_page - 1) // per_page\n",
    "    \n",
    "    print(f\"Fetching {limit} works across {pages_needed} pages...\")\n",
    "    \n",
    "    for page in range(1, pages_needed + 1):\n",
    "        params = {\n",
    "            \"per-page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_string = \",\".join([f\"{k}:{v}\" for k, v in filters.items()])\n",
    "            params[\"filter\"] = filter_string\n",
    "        \n",
    "        response = make_openalex_request(\"works\", params)\n",
    "        \n",
    "        if not response or \"results\" not in response:\n",
    "            print(f\"Failed to fetch page {page}\")\n",
    "            break\n",
    "        \n",
    "        # Extract relevant fields from each work\n",
    "        for work in response[\"results\"]:\n",
    "            work_data = {\n",
    "                \"id\": work.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\"),\n",
    "                \"doi\": work.get(\"doi\"),\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"publication_year\": work.get(\"publication_year\"),\n",
    "                \"publication_date\": work.get(\"publication_date\"),\n",
    "                \"type\": work.get(\"type\"),\n",
    "                \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
    "                \"is_open_access\": work.get(\"open_access\", {}).get(\"is_oa\", False),\n",
    "                \"language\": work.get(\"language\"),\n",
    "                \"primary_location\": work.get(\"primary_location\", {}).get(\"source\", {}).get(\"display_name\") if work.get(\"primary_location\") else None,\n",
    "                \"corresponding_author_ids\": [auth.get(\"author\", {}).get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                                           for auth in work.get(\"authorships\", []) if auth.get(\"is_corresponding\")],\n",
    "                \"institution_ids\": list(set([inst.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                                           for auth in work.get(\"authorships\", []) \n",
    "                                           for inst in auth.get(\"institutions\", [])])),\n",
    "                \"concept_ids\": [concept.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                              for concept in work.get(\"concepts\", [])],\n",
    "                \"abstract\": work.get(\"abstract\"),\n",
    "                \"abstract_inverted_index\": len(work.get(\"abstract_inverted_index\", {})) > 0\n",
    "            }\n",
    "            works_data.append(work_data)\n",
    "        \n",
    "        print(f\"Fetched page {page}/{pages_needed} - {len(works_data)} total works\")\n",
    "        \n",
    "        # Stop if we've reached the limit\n",
    "        if len(works_data) >= limit:\n",
    "            works_data = works_data[:limit]\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(works_data)\n",
    "\n",
    "# Example: Fetch recent AI/ML papers\n",
    "print(\"Example 1: Fetching recent AI/ML papers...\")\n",
    "ai_works = fetch_works(\n",
    "    filters={\n",
    "        \"concepts.id\": \"C154945302\",  # Artificial Intelligence concept ID\n",
    "        \"publication_year\": \"2023\",\n",
    "        \"type\": \"article\"\n",
    "    },\n",
    "    limit=50\n",
    ")\n",
    "\n",
    "print(f\"\\nFetched {len(ai_works)} AI papers:\")\n",
    "print(ai_works[[\"title\", \"publication_year\", \"cited_by_count\", \"is_open_access\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c8036",
   "metadata": {},
   "source": [
    "## 5. Fetch Authors Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_authors(filters: Dict = None, limit: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch author information from OpenAlex API.\n",
    "    \n",
    "    Args:\n",
    "        filters: Dictionary of filters to apply\n",
    "        limit: Maximum number of results to fetch\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with author data\n",
    "    \"\"\"\n",
    "    authors_data = []\n",
    "    per_page = min(25, limit)\n",
    "    pages_needed = (limit + per_page - 1) // per_page\n",
    "    \n",
    "    print(f\"Fetching {limit} authors across {pages_needed} pages...\")\n",
    "    \n",
    "    for page in range(1, pages_needed + 1):\n",
    "        params = {\n",
    "            \"per-page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_string = \",\".join([f\"{k}:{v}\" for k, v in filters.items()])\n",
    "            params[\"filter\"] = filter_string\n",
    "        \n",
    "        response = make_openalex_request(\"authors\", params)\n",
    "        \n",
    "        if not response or \"results\" not in response:\n",
    "            print(f\"Failed to fetch page {page}\")\n",
    "            break\n",
    "        \n",
    "        # Extract relevant fields from each author\n",
    "        for author in response[\"results\"]:\n",
    "            # Get current affiliation\n",
    "            current_affiliation = None\n",
    "            if author.get(\"affiliations\"):\n",
    "                current_affiliation = author[\"affiliations\"][0].get(\"institution\", {}).get(\"display_name\")\n",
    "            \n",
    "            author_data = {\n",
    "                \"id\": author.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\"),\n",
    "                \"orcid\": author.get(\"orcid\"),\n",
    "                \"display_name\": author.get(\"display_name\"),\n",
    "                \"display_name_alternatives\": author.get(\"display_name_alternatives\", []),\n",
    "                \"works_count\": author.get(\"works_count\", 0),\n",
    "                \"cited_by_count\": author.get(\"cited_by_count\", 0),\n",
    "                \"h_index\": author.get(\"summary_stats\", {}).get(\"h_index\", 0),\n",
    "                \"i10_index\": author.get(\"summary_stats\", {}).get(\"i10_index\", 0),\n",
    "                \"current_affiliation\": current_affiliation,\n",
    "                \"affiliation_history\": [aff.get(\"institution\", {}).get(\"display_name\") \n",
    "                                      for aff in author.get(\"affiliations\", [])],\n",
    "                \"concept_ids\": [concept.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                              for concept in author.get(\"x_concepts\", [])],\n",
    "                \"last_known_institution\": author.get(\"last_known_institution\", {}).get(\"display_name\"),\n",
    "                \"first_publication_year\": author.get(\"summary_stats\", {}).get(\"2yr_mean_citedness\"),\n",
    "                \"two_year_mean_citedness\": author.get(\"summary_stats\", {}).get(\"2yr_mean_citedness\")\n",
    "            }\n",
    "            authors_data.append(author_data)\n",
    "        \n",
    "        print(f\"Fetched page {page}/{pages_needed} - {len(authors_data)} total authors\")\n",
    "        \n",
    "        # Stop if we've reached the limit\n",
    "        if len(authors_data) >= limit:\n",
    "            authors_data = authors_data[:limit]\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(authors_data)\n",
    "\n",
    "# Example: Fetch highly cited authors in computer science\n",
    "print(\"Example 2: Fetching highly cited computer science authors...\")\n",
    "cs_authors = fetch_authors(\n",
    "    filters={\n",
    "        \"x_concepts.id\": \"C41008148\",  # Computer Science concept ID\n",
    "        \"cited_by_count\": \">1000\"\n",
    "    },\n",
    "    limit=25\n",
    ")\n",
    "\n",
    "print(f\"\\nFetched {len(cs_authors)} highly cited CS authors:\")\n",
    "print(cs_authors[[\"display_name\", \"works_count\", \"cited_by_count\", \"h_index\", \"current_affiliation\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae1498e",
   "metadata": {},
   "source": [
    "## 6. Fetch Institutions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ed2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_institutions(filters: Dict = None, limit: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch institution information from OpenAlex API.\n",
    "    \n",
    "    Args:\n",
    "        filters: Dictionary of filters to apply\n",
    "        limit: Maximum number of results to fetch\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with institution data\n",
    "    \"\"\"\n",
    "    institutions_data = []\n",
    "    per_page = min(25, limit)\n",
    "    pages_needed = (limit + per_page - 1) // per_page\n",
    "    \n",
    "    print(f\"Fetching {limit} institutions across {pages_needed} pages...\")\n",
    "    \n",
    "    for page in range(1, pages_needed + 1):\n",
    "        params = {\n",
    "            \"per-page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_string = \",\".join([f\"{k}:{v}\" for k, v in filters.items()])\n",
    "            params[\"filter\"] = filter_string\n",
    "        \n",
    "        response = make_openalex_request(\"institutions\", params)\n",
    "        \n",
    "        if not response or \"results\" not in response:\n",
    "            print(f\"Failed to fetch page {page}\")\n",
    "            break\n",
    "        \n",
    "        # Extract relevant fields from each institution\n",
    "        for institution in response[\"results\"]:\n",
    "            geo = institution.get(\"geo\", {})\n",
    "            \n",
    "            institution_data = {\n",
    "                \"id\": institution.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\"),\n",
    "                \"ror\": institution.get(\"ror\"),\n",
    "                \"display_name\": institution.get(\"display_name\"),\n",
    "                \"country_code\": institution.get(\"country_code\"),\n",
    "                \"type\": institution.get(\"type\"),\n",
    "                \"homepage_url\": institution.get(\"homepage_url\"),\n",
    "                \"image_url\": institution.get(\"image_url\"),\n",
    "                \"works_count\": institution.get(\"works_count\", 0),\n",
    "                \"cited_by_count\": institution.get(\"cited_by_count\", 0),\n",
    "                \"h_index\": institution.get(\"summary_stats\", {}).get(\"h_index\", 0),\n",
    "                \"i10_index\": institution.get(\"summary_stats\", {}).get(\"i10_index\", 0),\n",
    "                \"two_year_mean_citedness\": institution.get(\"summary_stats\", {}).get(\"2yr_mean_citedness\"),\n",
    "                \"city\": geo.get(\"city\"),\n",
    "                \"region\": geo.get(\"region\"),\n",
    "                \"country\": geo.get(\"country\"),\n",
    "                \"latitude\": geo.get(\"latitude\"),\n",
    "                \"longitude\": geo.get(\"longitude\"),\n",
    "                \"associated_institutions\": [assoc.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                                          for assoc in institution.get(\"associated_institutions\", [])],\n",
    "                \"concept_ids\": [concept.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\") \n",
    "                              for concept in institution.get(\"x_concepts\", [])]\n",
    "            }\n",
    "            institutions_data.append(institution_data)\n",
    "        \n",
    "        print(f\"Fetched page {page}/{pages_needed} - {len(institutions_data)} total institutions\")\n",
    "        \n",
    "        # Stop if we've reached the limit\n",
    "        if len(institutions_data) >= limit:\n",
    "            institutions_data = institutions_data[:limit]\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(institutions_data)\n",
    "\n",
    "# Example: Fetch top research universities\n",
    "print(\"Example 3: Fetching top research universities...\")\n",
    "top_universities = fetch_institutions(\n",
    "    filters={\n",
    "        \"type\": \"education\",\n",
    "        \"works_count\": \">10000\"\n",
    "    },\n",
    "    limit=20\n",
    ")\n",
    "\n",
    "print(f\"\\nFetched {len(top_universities)} top universities:\")\n",
    "print(top_universities[[\"display_name\", \"country\", \"works_count\", \"cited_by_count\", \"h_index\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f405f3",
   "metadata": {},
   "source": [
    "## 7. Handle Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb684e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_results_with_cursor(endpoint: str, filters: Dict = None, max_results: int = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch all results using cursor-based pagination for large datasets.\n",
    "    \n",
    "    Args:\n",
    "        endpoint: API endpoint ('works', 'authors', 'institutions')\n",
    "        filters: Dictionary of filters to apply\n",
    "        max_results: Maximum number of results to fetch (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of all results\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    cursor = \"*\"  # Start with initial cursor\n",
    "    per_page = 200  # Maximum allowed per page\n",
    "    \n",
    "    print(f\"Fetching all results from {endpoint} endpoint...\")\n",
    "    \n",
    "    while cursor and (max_results is None or len(all_results) < max_results):\n",
    "        params = {\n",
    "            \"per-page\": per_page,\n",
    "            \"cursor\": cursor\n",
    "        }\n",
    "        \n",
    "        # Add filters if provided\n",
    "        if filters:\n",
    "            filter_string = \",\".join([f\"{k}:{v}\" for k, v in filters.items()])\n",
    "            params[\"filter\"] = filter_string\n",
    "        \n",
    "        response = make_openalex_request(endpoint, params)\n",
    "        \n",
    "        if not response or \"results\" not in response:\n",
    "            print(f\"Failed to fetch results\")\n",
    "            break\n",
    "        \n",
    "        # Add results to our collection\n",
    "        batch_results = response[\"results\"]\n",
    "        if max_results:\n",
    "            remaining = max_results - len(all_results)\n",
    "            batch_results = batch_results[:remaining]\n",
    "        \n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Get next cursor\n",
    "        meta = response.get(\"meta\", {})\n",
    "        cursor = meta.get(\"next_cursor\")\n",
    "        \n",
    "        print(f\"Fetched {len(batch_results)} results - Total: {len(all_results)}\")\n",
    "        \n",
    "        # Stop if no more pages or we've reached max\n",
    "        if not cursor or (max_results and len(all_results) >= max_results):\n",
    "            break\n",
    "    \n",
    "    print(f\"Completed fetching {len(all_results)} total results\")\n",
    "    return all_results\n",
    "\n",
    "# Example: Fetch a large dataset of recent open access papers\n",
    "print(\"Example 4: Fetching large dataset with cursor pagination...\")\n",
    "print(\"Note: This will take a while and fetch many results. Adjust max_results as needed.\")\n",
    "\n",
    "# Fetch 1000 recent open access papers (you can increase this number)\n",
    "oa_papers = fetch_all_results_with_cursor(\n",
    "    endpoint=\"works\",\n",
    "    filters={\n",
    "        \"is_oa\": \"true\",\n",
    "        \"publication_year\": \"2023\",\n",
    "        \"type\": \"article\"\n",
    "    },\n",
    "    max_results=1000  # Adjust this number based on your needs\n",
    ")\n",
    "\n",
    "print(f\"Fetched {len(oa_papers)} open access papers from 2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42981a2",
   "metadata": {},
   "source": [
    "## 8. Data Processing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d788f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_works_data(raw_works: List[Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process raw works data into a clean DataFrame with expanded nested fields.\n",
    "    \n",
    "    Args:\n",
    "        raw_works: List of raw work dictionaries from API\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned DataFrame with processed fields\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    print(f\"Processing {len(raw_works)} works...\")\n",
    "    \n",
    "    for work in raw_works:\n",
    "        # Basic fields\n",
    "        processed_work = {\n",
    "            \"openalex_id\": work.get(\"id\", \"\").replace(\"https://openalex.org/\", \"\"),\n",
    "            \"doi\": work.get(\"doi\"),\n",
    "            \"title\": work.get(\"title\"),\n",
    "            \"publication_year\": work.get(\"publication_year\"),\n",
    "            \"publication_date\": work.get(\"publication_date\"),\n",
    "            \"type\": work.get(\"type\"),\n",
    "            \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
    "            \"language\": work.get(\"language\"),\n",
    "        }\n",
    "        \n",
    "        # Open access information\n",
    "        oa_info = work.get(\"open_access\", {})\n",
    "        processed_work.update({\n",
    "            \"is_oa\": oa_info.get(\"is_oa\", False),\n",
    "            \"oa_url\": oa_info.get(\"oa_url\"),\n",
    "            \"any_repository_has_fulltext\": oa_info.get(\"any_repository_has_fulltext\", False)\n",
    "        })\n",
    "        \n",
    "        # Primary location (journal/venue)\n",
    "        primary_location = work.get(\"primary_location\", {})\n",
    "        if primary_location:\n",
    "            source = primary_location.get(\"source\", {})\n",
    "            processed_work.update({\n",
    "                \"journal_name\": source.get(\"display_name\"),\n",
    "                \"journal_issn_l\": source.get(\"issn_l\"),\n",
    "                \"journal_is_oa\": source.get(\"is_oa\", False),\n",
    "                \"journal_type\": source.get(\"type\")\n",
    "            })\n",
    "        \n",
    "        # Authors information\n",
    "        authorships = work.get(\"authorships\", [])\n",
    "        processed_work.update({\n",
    "            \"author_count\": len(authorships),\n",
    "            \"author_names\": [auth.get(\"author\", {}).get(\"display_name\") for auth in authorships],\n",
    "            \"corresponding_author_count\": sum(1 for auth in authorships if auth.get(\"is_corresponding\")),\n",
    "            \"institution_count\": len(set([inst.get(\"id\") for auth in authorships for inst in auth.get(\"institutions\", []) if inst.get(\"id\")]))\n",
    "        })\n",
    "        \n",
    "        # Concepts (subject areas)\n",
    "        concepts = work.get(\"concepts\", [])\n",
    "        if concepts:\n",
    "            # Get top 3 concepts by score\n",
    "            top_concepts = sorted(concepts, key=lambda x: x.get(\"score\", 0), reverse=True)[:3]\n",
    "            processed_work.update({\n",
    "                \"primary_concept\": top_concepts[0].get(\"display_name\") if top_concepts else None,\n",
    "                \"concept_score\": top_concepts[0].get(\"score\") if top_concepts else None,\n",
    "                \"all_concepts\": [c.get(\"display_name\") for c in concepts]\n",
    "            })\n",
    "        \n",
    "        # Abstract information\n",
    "        processed_work.update({\n",
    "            \"has_abstract\": bool(work.get(\"abstract\")),\n",
    "            \"abstract_length\": len(work.get(\"abstract\", \"\")),\n",
    "            \"has_inverted_abstract\": len(work.get(\"abstract_inverted_index\", {})) > 0\n",
    "        })\n",
    "        \n",
    "        processed_data.append(processed_work)\n",
    "    \n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Convert date columns\n",
    "    if \"publication_date\" in df.columns:\n",
    "        df[\"publication_date\"] = pd.to_datetime(df[\"publication_date\"], errors=\"coerce\")\n",
    "    \n",
    "    print(f\"Processed {len(df)} works successfully\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_data_quality(df: pd.DataFrame, entity_type: str = \"works\") -> None:\n",
    "    \"\"\"\n",
    "    Analyze data quality and completeness of the fetched data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        entity_type: Type of entities ('works', 'authors', 'institutions')\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Data Quality Analysis for {entity_type.title()} ===\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if missing[col] > 0:\n",
    "            print(f\"  {col}: {missing[col]} ({missing_pct[col]:.1f}%)\")\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nNumeric column statistics:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate records: {duplicates}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "# Example: Process the AI works we fetched earlier\n",
    "if 'ai_works' in locals() and len(ai_works) > 0:\n",
    "    print(\"Processing AI works data...\")\n",
    "    # Convert to the raw format expected by the processing function\n",
    "    ai_works_raw = []\n",
    "    for _, row in ai_works.iterrows():\n",
    "        # Create a mock raw work structure for processing\n",
    "        raw_work = {\n",
    "            \"id\": f\"https://openalex.org/{row['id']}\",\n",
    "            \"title\": row[\"title\"],\n",
    "            \"publication_year\": row[\"publication_year\"],\n",
    "            \"cited_by_count\": row[\"cited_by_count\"],\n",
    "            \"open_access\": {\"is_oa\": row[\"is_open_access\"]},\n",
    "            \"type\": row[\"type\"]\n",
    "        }\n",
    "        ai_works_raw.append(raw_work)\n",
    "    \n",
    "    processed_ai_works = process_works_data(ai_works_raw)\n",
    "    analyze_data_quality(processed_ai_works, \"AI works\")\n",
    "else:\n",
    "    print(\"No AI works data available for processing. Run the fetch_works cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773710c9",
   "metadata": {},
   "source": [
    "## 9. Export Data to Different Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def export_data(df: pd.DataFrame, entity_type: str, base_filename: str = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Export DataFrame to multiple formats (CSV, JSON, Excel).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to export\n",
    "        entity_type: Type of entities ('works', 'authors', 'institutions')\n",
    "        base_filename: Base filename (will add timestamp if None)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping format to filepath\n",
    "    \"\"\"\n",
    "    if base_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"openalex_{entity_type}_{timestamp}\"\n",
    "    \n",
    "    # Create data directory if it doesn't exist\n",
    "    data_dir = \"/home/dnhoa/IAAIR/data\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    exported_files = {}\n",
    "    \n",
    "    # Export to CSV\n",
    "    csv_path = os.path.join(data_dir, f\"{base_filename}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    exported_files[\"csv\"] = csv_path\n",
    "    print(f\"✅ Exported to CSV: {csv_path}\")\n",
    "    \n",
    "    # Export to JSON\n",
    "    json_path = os.path.join(data_dir, f\"{base_filename}.json\")\n",
    "    df.to_json(json_path, orient=\"records\", indent=2)\n",
    "    exported_files[\"json\"] = json_path\n",
    "    print(f\"✅ Exported to JSON: {json_path}\")\n",
    "    \n",
    "    # Export to Excel (if openpyxl is available)\n",
    "    try:\n",
    "        excel_path = os.path.join(data_dir, f\"{base_filename}.xlsx\")\n",
    "        df.to_excel(excel_path, index=False, engine='openpyxl')\n",
    "        exported_files[\"excel\"] = excel_path\n",
    "        print(f\"✅ Exported to Excel: {excel_path}\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️  Excel export skipped (install openpyxl for Excel support)\")\n",
    "    \n",
    "    return exported_files\n",
    "\n",
    "def create_data_summary(datasets: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a summary of all collected datasets.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary mapping dataset name to DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        Summary DataFrame\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        summary = {\n",
    "            \"dataset\": name,\n",
    "            \"records\": len(df),\n",
    "            \"columns\": len(df.columns),\n",
    "            \"memory_usage_mb\": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),\n",
    "            \"date_range\": \"\",\n",
    "            \"key_columns\": list(df.columns[:5])  # First 5 columns\n",
    "        }\n",
    "        \n",
    "        # Try to get date range if publication_year exists\n",
    "        if \"publication_year\" in df.columns:\n",
    "            years = df[\"publication_year\"].dropna()\n",
    "            if len(years) > 0:\n",
    "                summary[\"date_range\"] = f\"{years.min()}-{years.max()}\"\n",
    "        \n",
    "        summary_data.append(summary)\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Example: Export all the data we've collected\n",
    "datasets_to_export = {}\n",
    "\n",
    "# Add datasets if they exist\n",
    "if 'ai_works' in locals():\n",
    "    datasets_to_export['ai_works'] = ai_works\n",
    "if 'cs_authors' in locals():\n",
    "    datasets_to_export['cs_authors'] = cs_authors\n",
    "if 'top_universities' in locals():\n",
    "    datasets_to_export['top_universities'] = top_universities\n",
    "\n",
    "print(\"Exporting collected datasets...\")\n",
    "exported_files_summary = {}\n",
    "\n",
    "for dataset_name, df in datasets_to_export.items():\n",
    "    print(f\"\\nExporting {dataset_name} ({len(df)} records)...\")\n",
    "    files = export_data(df, dataset_name)\n",
    "    exported_files_summary[dataset_name] = files\n",
    "\n",
    "# Create and export summary\n",
    "if datasets_to_export:\n",
    "    print(\"\\nCreating data summary...\")\n",
    "    summary_df = create_data_summary(datasets_to_export)\n",
    "    summary_files = export_data(summary_df, \"summary\", \"data_collection_summary\")\n",
    "    \n",
    "    print(\"\\n=== Data Collection Summary ===\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n=== All Exported Files ===\")\n",
    "    for dataset, files in exported_files_summary.items():\n",
    "        print(f\"{dataset}:\")\n",
    "        for format_type, filepath in files.items():\n",
    "            print(f\"  {format_type.upper()}: {filepath}\")\n",
    "else:\n",
    "    print(\"No datasets available for export. Run the data fetching cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1e10d",
   "metadata": {},
   "source": [
    "## 10. Advanced Queries and Use Cases\n",
    "\n",
    "Here are some advanced query examples for specific research tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Query Examples\n",
    "\n",
    "# 1. COVID-19 research impact analysis\n",
    "def analyze_covid_research():\n",
    "    \"\"\"Analyze COVID-19 research output and impact\"\"\"\n",
    "    covid_params = {\n",
    "        \"filter\": \"concepts.id:C2778407487,publication_year:>2019\",  # COVID-19 concept\n",
    "        \"per-page\": 50\n",
    "    }\n",
    "    \n",
    "    response = make_openalex_request(\"works\", covid_params)\n",
    "    if response:\n",
    "        print(f\"COVID-19 papers since 2020: {response['meta']['count']:,}\")\n",
    "        \n",
    "        # Analyze top papers by citations\n",
    "        papers = response[\"results\"]\n",
    "        top_papers = sorted(papers, key=lambda x: x.get(\"cited_by_count\", 0), reverse=True)[:5]\n",
    "        \n",
    "        print(\"\\nTop 5 most cited COVID-19 papers:\")\n",
    "        for i, paper in enumerate(top_papers, 1):\n",
    "            title = paper.get(\"title\", \"No title\")[:60] + \"...\"\n",
    "            citations = paper.get(\"cited_by_count\", 0)\n",
    "            year = paper.get(\"publication_year\")\n",
    "            print(f\"{i}. {title} ({year}) - {citations:,} citations\")\n",
    "\n",
    "# 2. Collaboration network analysis\n",
    "def analyze_international_collaboration():\n",
    "    \"\"\"Analyze international collaboration patterns\"\"\"\n",
    "    collab_params = {\n",
    "        \"filter\": \"authorships.institutions.country_code:US,authorships.institutions.country_code:!US,publication_year:2023\",\n",
    "        \"per-page\": 25\n",
    "    }\n",
    "    \n",
    "    response = make_openalex_request(\"works\", collab_params)\n",
    "    if response:\n",
    "        print(f\"US international collaborations in 2023: {response['meta']['count']:,}\")\n",
    "\n",
    "# 3. Open Science analysis\n",
    "def analyze_open_science_trends():\n",
    "    \"\"\"Analyze open access and open science trends\"\"\"\n",
    "    # Compare OA vs non-OA papers by citation impact\n",
    "    oa_params = {\n",
    "        \"filter\": \"is_oa:true,publication_year:2022\",\n",
    "        \"per-page\": 100\n",
    "    }\n",
    "    \n",
    "    non_oa_params = {\n",
    "        \"filter\": \"is_oa:false,publication_year:2022\", \n",
    "        \"per-page\": 100\n",
    "    }\n",
    "    \n",
    "    oa_response = make_openalex_request(\"works\", oa_params)\n",
    "    non_oa_response = make_openalex_request(\"works\", non_oa_params)\n",
    "    \n",
    "    if oa_response and non_oa_response:\n",
    "        oa_citations = [w.get(\"cited_by_count\", 0) for w in oa_response[\"results\"]]\n",
    "        non_oa_citations = [w.get(\"cited_by_count\", 0) for w in non_oa_response[\"results\"]]\n",
    "        \n",
    "        print(\"Open Access vs Non-Open Access Citation Analysis (2022):\")\n",
    "        print(f\"OA papers average citations: {sum(oa_citations)/len(oa_citations):.2f}\")\n",
    "        print(f\"Non-OA papers average citations: {sum(non_oa_citations)/len(non_oa_citations):.2f}\")\n",
    "\n",
    "# 4. Emerging research topics\n",
    "def find_emerging_topics():\n",
    "    \"\"\"Find rapidly growing research areas\"\"\"\n",
    "    # Look for concepts with high growth in recent years\n",
    "    concepts_params = {\n",
    "        \"filter\": \"works_count:>1000\",\n",
    "        \"sort\": \"works_count:desc\",\n",
    "        \"per-page\": 20\n",
    "    }\n",
    "    \n",
    "    response = make_openalex_request(\"concepts\", concepts_params)\n",
    "    if response:\n",
    "        print(\"Top 10 research concepts by total works:\")\n",
    "        for i, concept in enumerate(response[\"results\"][:10], 1):\n",
    "            name = concept.get(\"display_name\", \"Unknown\")\n",
    "            count = concept.get(\"works_count\", 0)\n",
    "            level = concept.get(\"level\", 0)\n",
    "            print(f\"{i}. {name} (Level {level}) - {count:,} works\")\n",
    "\n",
    "# 5. Author productivity analysis\n",
    "def analyze_author_productivity():\n",
    "    \"\"\"Analyze author productivity patterns\"\"\"\n",
    "    productive_authors_params = {\n",
    "        \"filter\": \"works_count:>100,last_known_institution.country_code:US\",\n",
    "        \"sort\": \"cited_by_count:desc\",\n",
    "        \"per-page\": 10\n",
    "    }\n",
    "    \n",
    "    response = make_openalex_request(\"authors\", productive_authors_params)\n",
    "    if response:\n",
    "        print(\"Top 10 most cited US-based prolific authors:\")\n",
    "        for i, author in enumerate(response[\"results\"], 1):\n",
    "            name = author.get(\"display_name\", \"Unknown\")\n",
    "            works = author.get(\"works_count\", 0)\n",
    "            citations = author.get(\"cited_by_count\", 0)\n",
    "            h_index = author.get(\"summary_stats\", {}).get(\"h_index\", 0)\n",
    "            affiliation = author.get(\"last_known_institution\", {}).get(\"display_name\", \"Unknown\")\n",
    "            \n",
    "            print(f\"{i}. {name} - {works} works, {citations:,} citations, h-index: {h_index}\")\n",
    "            print(f\"   Affiliation: {affiliation}\")\n",
    "\n",
    "# Run example analyses\n",
    "print(\"=== Advanced OpenAlex Analysis Examples ===\\n\")\n",
    "\n",
    "print(\"1. COVID-19 Research Analysis:\")\n",
    "analyze_covid_research()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. International Collaboration:\")\n",
    "analyze_international_collaboration()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. Open Science Trends:\")\n",
    "analyze_open_science_trends()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"4. Top Research Concepts:\")\n",
    "find_emerging_topics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"5. Author Productivity:\")\n",
    "analyze_author_productivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7453d",
   "metadata": {},
   "source": [
    "## 11. Next Steps and Resources\n",
    "\n",
    "### Additional OpenAlex Features to Explore:\n",
    "- **Concepts**: Hierarchical subject classification system\n",
    "- **Sources**: Journals, repositories, and other publication venues  \n",
    "- **Publishers**: Academic publishers and their metadata\n",
    "- **Funders**: Research funding organizations and grant information\n",
    "\n",
    "### Best Practices:\n",
    "1. **Rate Limiting**: Always respect API rate limits (1 req/sec for polite pool)\n",
    "2. **Email Registration**: Add your email to get higher rate limits\n",
    "3. **Caching**: Cache results for repeated analysis\n",
    "4. **Batch Processing**: Use cursor pagination for large datasets\n",
    "5. **Error Handling**: Implement robust error handling for production use\n",
    "\n",
    "### Useful Resources:\n",
    "- [OpenAlex Documentation](https://docs.openalex.org/)\n",
    "- [API Overview](https://docs.openalex.org/how-to-use-the-api/api-overview)\n",
    "- [Filter Documentation](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists)\n",
    "- [OpenAlex Community](https://groups.google.com/g/openalex-users)\n",
    "\n",
    "### Integration with Knowledge Fabric:\n",
    "This data can be directly integrated into your Knowledge Fabric system using the ingestion pipeline in `src/knowledge_fabric/ingestion/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac8b0e",
   "metadata": {},
   "source": [
    "## 12. Finding OpenAlex IDs\n",
    "\n",
    "Here are practical examples of how to discover OpenAlex IDs for different entities:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
